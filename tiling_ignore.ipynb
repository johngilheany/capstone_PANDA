{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "702f8d6d-4bc1-4379-8ce5-d2d659eee3a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/florisvanwettum/panda-submission\n",
    "# To dos: \n",
    "# 1. Tile images (256x256)\n",
    "# 2. Filter images (i.e. remove ones with mostly white space)\n",
    "# 3. Mask - red/green for cancer/non cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd6dc1d-2009-4b5d-b523-4861ba639976",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as pl\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# The path can also be read from a config file, etc.\n",
    "OPENSLIDE_PATH = r'C:\\Users\\johng\\Downloads\\openslide-win64-20231011\\bin'\n",
    "\n",
    "import os\n",
    "if hasattr(os, 'add_dll_directory'):\n",
    "    # Windows\n",
    "    with os.add_dll_directory(OPENSLIDE_PATH):\n",
    "        import openslide\n",
    "else:\n",
    "    import openslide\n",
    "    \n",
    "from tqdm.notebook import tqdm\n",
    "import zipfile\n",
    "import timm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dfdb8d0-38b3-4875-8473-c802c24e9c57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an empty submission.csv for Kaggle to recognise\n",
    "with open('submission.csv', 'w') as submis:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0f2d550-864d-45da-b156-6dbd85628be4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd9a2201-e12f-4a9d-adb5-a4876e26cc14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tiler():\n",
    "    def __init__(self, N_tiles= 36, tile_size = 2**8, level=1):\n",
    "        self.N_tiles = N_tiles\n",
    "        self.tile_size = tile_size\n",
    "        self.level = level\n",
    "        \n",
    "    # This function takes an openslide object and returns the top left coordinates of N tiles (of a given size) with the most tissue pixels. \n",
    "    # Note: slide.level_dimensions[level] = (width,height).\n",
    "    # Note: padding is done to the right and bottom, this is to keep it simple while having at most 1 tile in memory at a time.\n",
    "    def _get_tile_locations_from_slide(self, slide):\n",
    "        tiles = []\n",
    "        required_padding = False\n",
    "        xlocs, ylocs = np.arange(0, slide.level_dimensions[self.level][0], self.tile_size), np.arange(0, slide.level_dimensions[self.level][1], self.tile_size) # Get the coordinates of the top left corners of the tiles.\n",
    "        for x_i, xloc in enumerate(xlocs):\n",
    "            for y_i, yloc in enumerate(ylocs):\n",
    "                region = np.copy(slide.read_region((xloc*(4**self.level),yloc*(4**self.level)), self.level, (self.tile_size,self.tile_size))) # The position is wrt. level 0, so must convert to level 0 coordinates by multiplying by the downsampling factor.\n",
    "                region_arr = np.asarray(region)[:,:,:3] # Ignore the alpha channel\n",
    "                if xloc+self.tile_size > slide.level_dimensions[self.level][0] or yloc+self.tile_size > slide.level_dimensions[self.level][1]: # if the tile goes out of bounds\n",
    "                    region_arr[region_arr==0] = 255\n",
    "                    required_padding = True\n",
    "                pixel_sum = region_arr.sum()\n",
    "                tiles.append({'xloc': xloc, 'yloc': yloc, 'pixel_sum': pixel_sum, 'required_padding': required_padding}) # store top left corner location and the tile's pixel_sum\n",
    "                required_padding = False\n",
    "        sorted_tiles = sorted(tiles, key= lambda d: d['pixel_sum']) # Sort tiles based on their pixel_sum field\n",
    "        sorted_tiles = sorted_tiles[:self.N_tiles] # Get top N tiles\n",
    "        return sorted_tiles\n",
    "    \n",
    "    # Return the tensor of individual tiles\n",
    "    def get_individual_tiles(self, slide, transform=None):\n",
    "        tiles_info = self._get_tile_locations_from_slide(slide)\n",
    "        tiles = torch.empty((self.N_tiles,3, self.tile_size, self.tile_size))\n",
    "        for i, tile in enumerate(tiles_info):\n",
    "            img = slide.read_region((tile['xloc']*(4**self.level),tile['yloc']*(4**self.level)), self.level, (self.tile_size,self.tile_size))\n",
    "            img = torch.clone(transforms.PILToTensor()(img)) # The position is wrt. level 0, so must convert to level 0 coordinates by multiplying by the downsampling factor.\n",
    "            img = img[:3,:,:] # Ignore the alpha channel\n",
    "            if tile['required_padding']:\n",
    "                img[img==0] = 255\n",
    "            if transforms: # SHOULD BE FASLE FOR TEST\n",
    "                img = img.float()/255.0 # Necessary for the transformations, images are expected to be a tensor with elements between [0,1]\n",
    "                img = transform(img)\n",
    "            tiles[i,...] = img\n",
    "        return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677be91b-735c-4a23-ab15-4acb850e8218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fea77553-f919-46cd-a52b-7390ca23514b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDataset(pl.LightningModule):\n",
    "    def __init__(self, df, dir_name):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.img_dir = f'/kaggle/input/prostate-cancer-grade-assessment/{dir_name}/'\n",
    "        self.tiler = Tiler(N_tiles = 36, tile_size=2**8, level=1)\n",
    "\n",
    "        self.normalize = transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "            \n",
    "    # Returns the individual tiles\n",
    "    def __getitem__(self, idx):\n",
    "            item_name = self.df.iloc[idx].loc['image_id'] # Get the name of the sample\n",
    "            file_path = os.path.join(self.img_dir, f'{item_name}.tiff')\n",
    "            slide = openslide.OpenSlide(file_path)\n",
    "            tiles = self.tiler.get_individual_tiles(slide, transform=self.normalize)\n",
    "            return tiles\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bab6110-5265-4dd1-b62a-146ac27bebb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(model, dataloader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    sigm = nn.Sigmoid()\n",
    "    threshold = 0.5\n",
    "    for i, img in enumerate(dataloader):\n",
    "        img = img.to(device).float()\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "            output = sigm(output)\n",
    "            output = torch.where(output>threshold, 1, 0)\n",
    "            pred = torch.sum(output, axis=1)\n",
    "        preds.append(pred.to('cpu').numpy()) # add the predictions of this batch to the overall list\n",
    "    preds = np.concatenate(preds) # Make it a single list of predictions over all batches\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "928e96bd-b290-45cb-aedc-b8de5de3214a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def submit(model, sample, dir_name='test_images'):\n",
    "    if os.path.exists(f'../input/prostate-cancer-grade-assessment/{dir_name}'):\n",
    "        print('run inference')\n",
    "        test_dataset = TestDataset(sample, dir_name)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "        preds = inference(model, test_loader, device)\n",
    "        sample['isup_grade'] = preds\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8859834b-2818-448c-a048-4e5f7317c097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # check using train_images\n",
    "# model = torch.jit.load('/kaggle/input/ismi-group3-panda-trained-models/convnext_pico_submission.pt')\n",
    "# submission = submit(model, train.head(), dir_name='train_images')\n",
    "# submission['isup_grade'] = submission['isup_grade'].astype(int)\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017fe5b5-cb8d-48ea-a41b-9118a8e8ba2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55c73dc-1498-4efe-8454-1455d7a9414f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969e615-4c6a-488e-bccc-de989f94e368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae85f61-8079-493f-bf63-b38efc95900c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
